# Multi-GPU training example

This example uses a PyTorch CNN adapted from https://github.com/jamespengcheng/PyTorch-CNN-on-CIFAR10/blob/master/ConvNetClassifier.py to demonstrate how one might parallelize model training across several GPUs on a single node. 

Note that this is a different problem than  spreading the model across multiple GPUs, as might be required when training very large models that cannot fit in the memory of a single GPU. 

Moreover, this example demonstrates that multi-GPU training does not necessarily lead to speedups in training, and some thought should be given to whether a model will benefit from parallel training, as well as how that parallelism is implemented. 

Several files contain commented code related to the machine learning resource Weights & Biases. This tool was used to profile the GPUs used during the runtime of this example, but is not officially endorsed by CHTC.
 
## File Explanations

- ```env.yml``` -- This file contains setup information for the conda environment needed by the job.

- ```http://proxy.chtc.wisc.edu/SQUID/gpu-examples/Miniconda3-latest-Linux-x86_64.sh``` -- The conda installation file located in the gpu-examples Squid directory.

- ```run_parallel.sh``` -- This file is run on the execute node. It handles environment setup and executes the python file ```model_parallel.py``` that trains the example neural net.

- ```submit.sub``` -- The submit file used by HTCondor for job submission.

- ```job_[cluster/process ID].log``` -- The HTCondor log file generated by this job.

- ```err_[cluster/process ID].err``` -- The job's stderr output. Returned upon the job's completion.

- ```out_[cluster/process ID].out``` -- The job's stdout output. This file will print the time taken by the model training loop on one GPU and can be compared to the output files for the other tests. Returned upon the job's completion.

- ```model_with_epoch[number].pth``` -- Model checkpoints generated by PyTorch. These files can be ignored. Returned upon the job's completion.


## GPU Training

To run this example, several choices must be made. First, decide how many GPUs should be requested. It is suggested choosing between 1-3 GPUs. While 4 GPUs is theoretically possible on a single node, requesting 4 GPUs may result in long queue times due to resource competition. Two files need to be updated to reflect this choice:

- ```submit.sub``` -- Line 19 
- ```run_parallel.sh``` -- Line 18 

After these files are updated, to submit the job run ```condor_submit submit.sub``` from your terminal. This will add the job to the queue.

When the job finishes, the job, err, out, and model checkpoint files will return to the submit node.

To compare model training times, compare the times listed in the output files.

