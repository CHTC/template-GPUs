# Multi-GPU training example

This example uses a PyTorch CNN adapted from https://github.com/jamespengcheng/PyTorch-CNN-on-CIFAR10/blob/master/ConvNetClassifier.py to demonstrate how one might parallelize model training across several GPUs on a single node.
Note that this is a different problem than spreading the model across multiple GPUs, as might be required when training very large models that cannot fit in the memory of a single GPU.
Moreover, this example demonstrates that multi-GPU training does not necessarily lead to speedups in training, and some thought should be given to whether a model will benefit from parallel training, as well as how that parallelism is implemented. 

Several files contain commented code related to the machine learning resource [Weights & Biases](https://wandb.ai/site).
This tool was used to profile the GPUs used during the development of this example but is not officially endorsed by CHTC.
 
## File Explanations

The following files are transferred to the job or created by the job and transferred back after the job completes:
- ```env.yml``` -- This file contains setup information for the conda environment needed by the job.
- ```http://proxy.chtc.wisc.edu/SQUID/gpu-examples/Miniconda3-latest-Linux-x86_64.sh``` -- The conda installation file located in the gpu-examples Squid directory.
- ```run_parallel.sh``` -- This file is run on the execute node. It handles environment setup and executes the Python script.
- ```model_parallel.py``` -- The Python script that trains the example neural net.
- ```submit.sub``` -- The submit file used by HTCondor for job submission.
- ```job_[cluster/process ID].log``` -- The HTCondor log file generated by this job.
- ```err_[cluster/process ID].err``` -- The job's stderr output. Returned upon the job's completion.
- ```out_[cluster/process ID].out``` -- The job's stdout output. This file will print the time taken by the model training loop on one GPU and can be compared to the output files for the other tests. Returned upon the job's completion.
- ```model_with_epoch[number].pth``` -- Model checkpoints generated by PyTorch. These files can be ignored. Returned upon the job's completion.

## GPU Training

To run this example, several choices must be made.
First, decide how many GPUs to request.
Choosing between 1-3 GPUs is recommended.
While 4 GPUs is theoretically possible on a single node, requesting 4 GPUs may result in long queue times due to resource competition.
One file needs to be updated to reflect this choice:

- ```submit.sub``` -- line 21 

After these files are updated, to submit the job run ```condor_submit submit.sub``` from your terminal.
This will add the job to the queue.

When the job finishes, the job, err, out, and model checkpoint files will return to the submit node.
To compare model training times, compare the times listed in the output files.

